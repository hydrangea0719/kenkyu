% kenkyukai_template.tex を編集

%\documentstyle[epsf,twocolumn]{jarticle}       %LaTeX2.09仕様
\documentclass[twocolumn]{jarticle}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%
%%  基本 バージョン
%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setlength{\topmargin}{-45pt}
%\setlength{\oddsidemargin}{0cm}
\setlength{\oddsidemargin}{-7.5mm}
%\setlength{\evensidemargin}{0cm}
\setlength{\textheight}{24.1cm}
%setlength{\textheight}{25cm}
\setlength{\textwidth}{17.4cm}
%\setlength{\textwidth}{172mm}
\setlength{\columnsep}{11mm}


%【節がかわるごとに(1.1)(1.2) …(2.1)(2.2)と数式番号をつけるとき】
%\makeatletter
%\renewcommand{\theequation}{%
%\thesection.\arabic{equation}} %\@addtoreset{equation}{section}
%\makeatother

%\renewcommand{\arraystretch}{0.95} 行間の設定

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{graphicx}   %pLaTeX2e仕様(要\documentstyle ->\documentclass)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\twocolumn[
\noindent

\hspace{1em}

情報工学実験I\hspace{-.1em}I
\hfill
\ \ B3 多田 瑞葵


\vspace{2mm}

\hrule

\begin{center}
{\Large \bf BERT を用いた深層言語処理における品詞推定の調査}
\end{center}


\hrule
\vspace{3mm}
]

\section{はじめに}
近年，機械学習は目覚ましい発展を遂げ，自然言語の分野においても大きな成果を上げている．特に自然言語処理モデルの一つである BERT は，多様な自然言語処理タスクで好成績を残しており，注目を集めている．
\par
本実験では，BERT を用いて深層言語処理（とは？）を行い，文法情報（多分これは品詞のこと）が処理に（？）どのような影響を与えるかを確かめる（調べる？）
% \par
% とりあえず二値分類から始める．
% BERT が賢すぎて嬉しい．

\section{要素技術}
% 要素技術と従来手法をあわせて要素技術

  \subsection{BERT}
  BERT \cite{DBLP} は Transformer による双方向のエンコーダーである．2008 年 10 月に Google の Jacob Devlin らの論文で発表された．
  % ここはもうちょっと BERT の紹介をする
  \par
  本実験では，東北大学が公開している訓練済み日本語 BERT モデルを使用する．
  % （もしかすると京都大学のやつを使うかもしれない）
  \par
  % BERT は一部の単語を [MASK] に置き換えることで，そこの単語を予測してくれます．そもそも BERT は二種類の事前学習をしていて，単語の予測と文の関係性の予測の二つを勉強していて，それによってとても賢いです．今回はその単語の予測の部分（BERT で事前学習している部分）を使うのですが，事前学習とタスクが同じなので，ファインチューニングが不要ですか...？（何もわかっていない顔）

  \subsection{MeCab}
  MeCab は，オープンソース形態素解析エンジンであり，京都大学情報学研究科-日本電信電話株式会社コミュニケーション科学基礎研究所共同研究ユニットプロジェクトを通して開発された．
  % （HPに書いてあったのをそのまま書いただけで，しかもこの〜〜プロジェクトのリンクが切れてるのでまたちゃんと調べておく）
  \par
  日本語は，文中で単語が区切りなく並べられているため，BERT を使う前に文を単語に区切る必要がある．
  % また，形態素解析器はたくさんあるのだけれど，本実験で使用する BERT は MeCab を用いたトークナイズ（＝形態素解析？）によって訓練されているため，同様に MeCab を使用する．
  \par
  % MeCab が頑張って分かち書きをしてくれました．ありがとう〜〜．また単語分割だけではなく形態素解析で品詞を抽出します．これにより特定の品詞のみに [MASK] をかけることができます．

\section{実験手法}
% 従来手法があるなら提案手法とする
\subsection{使用データセット}
毎日新聞のデータを使用した．2008 年から 2012 年までの，1 面，2 面，3 面記事を使用することになると思います．記号などは可能な限り取り除いた．


\subsection{実験手法}

% [MASK] ってかけるもの？置き換えるもの？統一する．
% \par
[MASK] がかかっているところの単語の品詞が当てれるかを調べたい．
\begin{itemize}
  \item 名詞に [MASK] をかけた場合の精度を見る
  \item 動詞に [MASK] をかけた場合の精度を見る
  \item 形容詞に [MASK] をかけた場合の精度を見る
  \item 機能語に [MASK] をかけた場合の精度を見る
  \item 上記の結果を比較する
\end{itemize}


\section{数値実験}
  \subsection{実験１}
  % パラメータが〜からころちん〜
  % MeCab の辞書の設定とかもここですか？

  \subsection{実験結果}
  % 実験結果が出ればここに書けますが，でなければ書けません．頑張ります．

  \subsection{考察}
  % うーん，ばなな！
  % \par
  \begin{itemize}
    \item どの品詞がうまく予測できたか
    \item なぜうまくいったと考えられるのか
    \item どの品詞がうまく予測できなかったか
    \item なぜうまくいかなかったのか
    \item じゃあどうすればうまくいくのか
  \end{itemize}
  っていうのを何かしら定量的にみないといけない？はずなので，比較や評価の方法を調べておく．


\section{まとめと今後の課題}
これからも頑張りたいです，まる．


% 参考文献出力
\bibliography{index}
\bibliographystyle{unsrt}


\end{document}
